{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, auc\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "# Suppress scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
    "\n",
    "# Import data (Source: https://www.kaggle.com/arashnic/imbalanced-data-practice)\n",
    "data = pd.read_csv('aug_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert that each row contains unique user_id\n",
    "assert len(data['id'].unique()) == data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect summary statistics of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inspect_cont_feat(feature):\n",
    "\n",
    "    # Inspect percent missing, mean, standard dev, min, and max of continuous feature\n",
    "    feature_stats = {\n",
    "        'pct_null': np.round(feature.isnull().sum() * 100 / len(feature), 2),\n",
    "        'mean': np.round(np.mean(feature), 2),\n",
    "        'sd': np.round(np.std(feature), 2),\n",
    "        'min': np.round(np.min(feature), 2),\n",
    "        'max': np.round(np.max(feature), 2)}\n",
    "    \n",
    "    # Print feature summary statistics\n",
    "    print('-'*40)\n",
    "    print('Feature: {}'.format(feature.name))\n",
    "    print('Percent null: {}, mean: {}, standard deviation: {}, min: {}, max: {}'.format(\n",
    "        feature_stats['pct_null'], feature_stats['mean'], feature_stats['sd'],\n",
    "        feature_stats['min'], feature_stats['max']))\n",
    "\n",
    "    \n",
    "def inspect_cat_feat(feature):\n",
    "\n",
    "    # Inspect percent missing and distribution of values for categorical feature\n",
    "    feature_stats = {\n",
    "        'pct_null': np.round(feature.isnull().sum() * 100 / len(feature), 2),\n",
    "        'dist': feature.value_counts()}\n",
    "    \n",
    "    # Print feature summary statistics\n",
    "    print('-'*40)\n",
    "    print('Feature: {}'.format(feature.name))\n",
    "    print('Percent null: {}'.format(feature_stats['pct_null']))\n",
    "    print('Distribution of values:')\n",
    "    print(feature_stats['dist'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inspect the continous features\n",
    "continous_feat = ['Age', 'Annual_Premium', 'Vintage']\n",
    "for feature in continous_feat:\n",
    "    inspect_cont_feat(data[feature])\n",
    "\n",
    "# Inspect the categorical features\n",
    "categorical_feat = ['Gender', 'Driving_License', 'Region_Code', 'Previously_Insured', \n",
    "                    'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\n",
    "for feature in categorical_feat:\n",
    "    inspect_cat_feat(data[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect outcome variable (missing values, distinct values, class imbalance)\n",
    "inspect_cat_feat(data['Response'])\n",
    "\n",
    "print('Interest variable, percent negative examples: {}, percent positive examples {}'.format(\n",
    "    np.round((sum(data['Response'] == 0) / data.shape[0]) * 100, 2),\n",
    "    np.round((sum(data['Response'] == 1) / data.shape[0]) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant features\n",
    "del data['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast binary string features to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Gender to numeric (Male => 0, Female => 1)\n",
    "data['Gender'] = (data['Gender'] == 'Female').astype(int)\n",
    "\n",
    "# Cast Vehicle_Damage to numeric (No => 0, Yes => 1)\n",
    "data['Vehicle_Damage'] = (data['Vehicle_Damage'] == 'Yes').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast ordinal string features to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Vehicle_Age to numeric ('< 1 Year' => 0, '1-2 Year' => 1, '> 2 Years' => 2)\n",
    "data.loc[data['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = '0'\n",
    "data.loc[data['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = '1'\n",
    "data.loc[data['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = '2'\n",
    "data['Vehicle_Age'] = data['Vehicle_Age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode_feat(data, categorical_feat):\n",
    "\n",
    "    preprocessed_data = pd.DataFrame()\n",
    "\n",
    "    for col_name in data.columns:\n",
    "        if col_name in categorical_feat:\n",
    "            # One-hot encode categorical features\n",
    "            one_hot_feat = pd.get_dummies(data[col_name])\n",
    "            one_hot_feat = pd.get_dummies(data[col_name], prefix=col_name)\n",
    "            preprocessed_data = pd.concat((preprocessed_data, one_hot_feat), axis=1)\n",
    "        else:\n",
    "            preprocessed_data = pd.concat((preprocessed_data, data[col_name]), axis=1)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "\n",
    "data = one_hot_encode_feat(data, ['Region_Code', 'Policy_Sales_Channel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns\n",
    "cols = ['Response'] \\\n",
    "    + [col for col in data.columns \\\n",
    "           if not (col.startswith('Region_Code_') or col.startswith('Policy_Sales_Channel_')) \\\n",
    "           and col != 'Response'] \\\n",
    "    + [col for col in data.columns if col.startswith('Region_Code_')] \\\n",
    "    + [col for col in data.columns if col.startswith('Policy_Sales_Channel_')]\n",
    "\n",
    "data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "data = data.sample(frac=1, random_state=1)\n",
    "\n",
    "# Assign features, outcome to separate dataframes\n",
    "X = data.copy()\n",
    "del X['Response']\n",
    "y = data['Response']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect distribution of outcome across train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If classes in outcome are imbalanced, inspect class distribution of train, validation, and test set\n",
    "label_counts = pd.DataFrame({\n",
    "    'data_set': ['train', 'validation', 'test'],\n",
    "    'percent_0': [np.round(np.mean(y), 5) for y in [y_train, y_val, y_test]],\n",
    "    'percent_1': [np.round((1.0 - np.mean(y)), 5) for y in [y_train, y_val, y_test]],\n",
    "    'count_0': [len(y[y == 0]) for y in [y_train, y_val, y_test]],\n",
    "    'count_1': [len(y[y == 1]) for y in [y_train, y_val, y_test]]})\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform min-max scaling for continous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continous_feat = ['Age', 'Annual_Premium', 'Vintage']\n",
    "\n",
    "# Pull summary statistics from training set for min-max scaling and missing value imputation\n",
    "train_col_max = X_train.max(axis=0)\n",
    "train_col_min = X_train.min(axis=0)\n",
    "train_col_mode = X_train.mode(axis=0).iloc[0, :]\n",
    "\n",
    "# Scale continuous features using min and max from training examples\n",
    "for data_split in [X_train, X_val, X_test]:\n",
    "    for feat in continous_feat:\n",
    "        data_split[feat] = (data_split[feat] - train_col_min[feat]) / (train_col_max[feat] - train_col_min[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect preprocessed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for data_split in [('Train', X_train), ('Validation', X_val), ('Test', X_test)]:\n",
    "    \n",
    "    data_split_name, data_split = data_split\n",
    "\n",
    "    print('-'*100)\n",
    "    print('Dataset: {}'.format(data_split_name))\n",
    "    \n",
    "    # Inspect preprocessed continuous features\n",
    "    for feature in ['Age', 'Annual_Premium', 'Vintage']:\n",
    "        inspect_cont_feat(data_split[feature])\n",
    "\n",
    "    # Inspect preprocessed categorical features\n",
    "    for feature in ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage']:\n",
    "        inspect_cat_feat(data_split[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect highly correlated features\n",
    "for i in range(X_train.shape[1]):\n",
    "    for j in range(i+1, X_train.shape[1]):\n",
    "        col1, col2 = X_train.columns[i], X_train.columns[j]\n",
    "        corr = np.round(X_train[col1].corr(X_train[col2]), 2)\n",
    "        if np.abs(corr) > 0.3:\n",
    "            print('Correlation between features {} and {} is {}'.format(col1, col2, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_metrics(y_true, y_hat):\n",
    "    # Return evaluation metrics\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(y_true, np.round(y_hat).astype(int), normalize='true'))\n",
    "    conf_mat.columns = ['predicted_0', 'predicted_1']\n",
    "    conf_mat.index = ['actual_0', 'actual_1']\n",
    "    \n",
    "    # Calculate the area under the curve of precision-recall plot\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_hat)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    return {\n",
    "        'f1_score': f1_score(y_true, np.round(y_hat).astype(int)),\n",
    "        'pr_auc': pr_auc,\n",
    "        'confusion_matrix': conf_mat}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear model\n",
    "lin_model = sklearn.linear_model.LogisticRegression(class_weight='balanced')\n",
    "lin_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect train, validation error metrics\n",
    "y_hat_train = lin_model.predict_proba(X_train)[:, 1]\n",
    "y_hat_val = lin_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "train_eval_metrics = eval_metrics(y_train, y_hat_train)\n",
    "val_eval_metrics = eval_metrics(y_val, y_hat_val)\n",
    "\n",
    "print('-'*40)\n",
    "print('Train evaluation metrics')\n",
    "print('F1 score: {}'.format(train_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(train_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(train_eval_metrics['confusion_matrix'])\n",
    "\n",
    "print('-'*40)\n",
    "print('Validation evaluation metrics')\n",
    "print('F1 score: {}'.format(val_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(val_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(val_eval_metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine linear relationships between features and outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually add bias term for linear model by adding col with constant values\n",
    "X_train_sm, X_val_sm = X_train.copy(), X_val.copy()\n",
    "X_train_sm.loc[:, 'const'] = 1.0\n",
    "\n",
    "# Train linear model for regression analysis\n",
    "sm_lin_model = sm.Logit(y_train, X_train_sm).fit()\n",
    "\n",
    "# Inspect coefficients and p-values for each features\n",
    "sm_lin_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_rf_model(X, y, hyperparameters):\n",
    "\n",
    "    rf_model = sklearn.ensemble.RandomForestClassifier(\n",
    "        n_estimators=hyperparameters['n_estimators'], \n",
    "        max_features=hyperparameters['max_features'],\n",
    "        max_depth=hyperparameters['max_depth'],\n",
    "        class_weight=hyperparameters['class_weight'],\n",
    "        bootstrap=hyperparameters['bootstrap'],\n",
    "        # Parralelize training across all CPUs\n",
    "        n_jobs=-1)\n",
    "\n",
    "    rf_model.fit(X, y)\n",
    "    \n",
    "    return rf_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "# Set default hyperparameter values\n",
    "default_hp_vals = {\n",
    "    'n_estimators': 100,\n",
    "    'max_features': 'auto',\n",
    "    'max_depth': 12,\n",
    "    'class_weight': 'balanced',\n",
    "    'bootstrap': True}\n",
    "\n",
    "# Define hyperparameters to test\n",
    "hp_test_vals = {\n",
    "    'n_estimators': [5, 10, 25, 50, 100, 150, 200, 500],\n",
    "    'max_features': ['auto', int(num_features * 0.5), int(num_features * 0.75), num_features],\n",
    "    'max_depth': [None, 1, 3, 5, 8, 12, 25, 50],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "    'bootstrap': [True, False]}\n",
    "\n",
    "# Store experimental results in dataframe\n",
    "rf_hp_tuning_results = pd.DataFrame(\n",
    "    columns=['n_estimators', 'max_features', 'max_depth', 'class_weight', 'bootstrap', \n",
    "             'train_f1_score', 'train_pr_curve_auc', 'val_f1_score', 'val_pr_curve_auc'])\n",
    "\n",
    "# Tune hyperparameters\n",
    "for hp in hp_test_vals.keys():\n",
    "    for hp_val in hp_test_vals[hp]:\n",
    "\n",
    "        # Define experiment hyperparameter values\n",
    "        exp_hp = default_hp_vals.copy()\n",
    "        exp_hp[hp] = hp_val\n",
    "        \n",
    "        # Train RF model\n",
    "        rf_model = train_rf_model(X_train, y_train, exp_hp)\n",
    "        \n",
    "        # Inspect train, validation error metrics\n",
    "        y_hat_train = rf_model.predict_proba(X_train)[:, 1]\n",
    "        y_hat_val = rf_model.predict_proba(X_val)[:, 1]\n",
    "        train_eval_metrics = eval_metrics(y_train, y_hat_train)\n",
    "        val_eval_metrics = eval_metrics(y_val, y_hat_val)\n",
    "        \n",
    "        # Record experiment metadata and eval metrics\n",
    "        exp_results = {\n",
    "            'n_estimators': exp_hp['n_estimators'],\n",
    "            'max_features': exp_hp['max_features'],\n",
    "            'max_depth': exp_hp['max_depth'],\n",
    "            'class_weight': exp_hp['class_weight'],\n",
    "            'bootstrap': exp_hp['bootstrap'],\n",
    "            'train_f1_score': train_eval_metrics['f1_score'],\n",
    "            'train_pr_curve_auc': train_eval_metrics['pr_auc'],\n",
    "            'val_f1_score': val_eval_metrics['f1_score'],\n",
    "            'val_pr_curve_auc': val_eval_metrics['pr_auc']}\n",
    "        rf_hp_tuning_results = rf_hp_tuning_results.append(\n",
    "            exp_results, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_hp_tuning_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train optimal random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimal hyperparameter values\n",
    "rf_optimal_hp = {\n",
    "    'n_estimators': 150,\n",
    "    'max_features': int(num_features * 0.75),\n",
    "    'max_depth': 12,\n",
    "    'class_weight': 'balanced_subsample',\n",
    "    'bootstrap': True}\n",
    "\n",
    "# Train RF model\n",
    "rf_model = train_rf_model(X_train, y_train, rf_optimal_hp)\n",
    "\n",
    "# Inspect train, validation error metrics\n",
    "y_hat_train = rf_model.predict_proba(X_train)[:, 1]\n",
    "y_hat_val = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "train_eval_metrics = eval_metrics(y_train, y_hat_train)\n",
    "val_eval_metrics = eval_metrics(y_val, y_hat_val)\n",
    "\n",
    "print('-'*40)\n",
    "print('Train evaluation metrics')\n",
    "print('F1 score: {}'.format(train_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(train_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(train_eval_metrics['confusion_matrix'])\n",
    "\n",
    "print('-'*40)\n",
    "print('Validation evaluation metrics')\n",
    "print('F1 score: {}'.format(val_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(val_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(val_eval_metrics['confusion_matrix'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine non-linear relationships between features and outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect variable importance\n",
    "rf_var_importance = pd.DataFrame({'feature': X_train.columns, 'importance': rf_model.feature_importances_})\n",
    "rf_var_importance.sort_values(by='importance', ascending=False, inplace=True)\n",
    "print(rf_var_importance.iloc[0:20, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Pandas dataframes to DMatrix class for XGBoost library\n",
    "train_xgb = xgb.DMatrix(X_train, label=y_train)\n",
    "val_xgb = xgb.DMatrix(X_val, label=y_val)\n",
    "test_xgb = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "def train_xgb_model(data, hyperparameters):\n",
    "    \n",
    "    # Define parameters for gradient boosting model\n",
    "    param = {\n",
    "        'objective':'binary:logistic',\n",
    "        'scale_pos_weight': 5.1,  # Set to heuristic sum(neg examples) / sum(pos examples)\n",
    "        'max_depth': hyperparameters['max_depth'], \n",
    "        'eta': hyperparameters['learning_rate'], \n",
    "        'max_delta_step': hyperparameters['max_delta_step'],\n",
    "        'colsample_bytree': hyperparameters['num_features']}\n",
    "    num_round = hyperparameters['n_round']\n",
    "\n",
    "    # Train model\n",
    "    xgb_model = xgb.train(param, data, num_round)\n",
    "\n",
    "    return xgb_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set default hyperparameter values\n",
    "default_hp_vals = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_round': 100,\n",
    "    'max_depth': 12,\n",
    "    'max_delta_step': 0,\n",
    "    'num_features': 1}\n",
    "\n",
    "# Define hyperparameter values to test\n",
    "hp_test_vals = {\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "    'n_round': [5, 10, 25, 50, 100, 150, 200, 500],\n",
    "    'max_depth': [1, 3, 5, 8, 12, 25], \n",
    "    'max_delta_step': [0, 1, 3, 5, 8, 10],\n",
    "    'num_features': [1.0, 0.75, 0.5, 0.25]}\n",
    "\n",
    "# Store experimental results in dataframe\n",
    "xgb_hp_tuning_results = pd.DataFrame(\n",
    "    columns=['learning_rate', 'n_round', 'max_depth', 'max_delta_step', 'num_features', \n",
    "             'train_f1_score', 'train_pr_curve_auc', 'val_f1_score', 'val_pr_curve_auc'])\n",
    "\n",
    "\n",
    "# Tune hyperparameters\n",
    "for hp in hp_test_vals.keys():\n",
    "    for hp_val in hp_test_vals[hp]:\n",
    "\n",
    "        # Define experiment hyperparameter values\n",
    "        exp_hp = default_hp_vals.copy()\n",
    "        exp_hp[hp] = hp_val\n",
    "        \n",
    "        # Train gradient boosting model\n",
    "        xgb_model = train_xgb_model(train_xgb, exp_hp)\n",
    "        \n",
    "        # Inspect train, validation error metrics\n",
    "        y_hat_train = xgb_model.predict(train_xgb)\n",
    "        y_hat_val = xgb_model.predict(val_xgb)\n",
    "        train_eval_metrics = eval_metrics(y_train, y_hat_train)\n",
    "        val_eval_metrics = eval_metrics(y_val, y_hat_val)\n",
    "        \n",
    "        # Record experiment metadata and eval metrics\n",
    "        exp_results = {\n",
    "            'learning_rate': exp_hp['learning_rate'],\n",
    "            'n_round': exp_hp['n_round'],\n",
    "            'max_depth': exp_hp['max_depth'],\n",
    "            'max_delta_step': exp_hp['max_delta_step'],\n",
    "            'num_features': exp_hp['num_features'],\n",
    "            'train_f1_score': train_eval_metrics['f1_score'],\n",
    "            'train_pr_curve_auc': train_eval_metrics['pr_auc'],\n",
    "            'val_f1_score': val_eval_metrics['f1_score'],\n",
    "            'val_pr_curve_auc': val_eval_metrics['pr_auc']}\n",
    "        xgb_hp_tuning_results = xgb_hp_tuning_results.append(\n",
    "            exp_results,\n",
    "            ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hp_tuning_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train optimal gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define optimal hyperparameter values\n",
    "xgb_optimal_hp = {\n",
    "    'learning_rate': .05,\n",
    "    'n_round': 150,\n",
    "    'max_depth': 12,\n",
    "    'max_delta_step': 0,\n",
    "    'num_features': 0.5}\n",
    "\n",
    "\n",
    "# Train XGB model\n",
    "xgb_model = train_xgb_model(train_xgb, xgb_optimal_hp)\n",
    "\n",
    "# Inspect train, validation error metrics\n",
    "y_hat_train = xgb_model.predict(train_xgb)\n",
    "y_hat_val = xgb_model.predict(val_xgb)\n",
    "train_eval_metrics = eval_metrics(y_train, y_hat_train)\n",
    "val_eval_metrics = eval_metrics(y_val, y_hat_val)\n",
    "\n",
    "print('-'*40)\n",
    "print('Train evaluation metrics')\n",
    "print('F1 score: {}'.format(train_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(train_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(train_eval_metrics['confusion_matrix'])\n",
    "\n",
    "print('-'*40)\n",
    "print('Validation evaluation metrics')\n",
    "print('F1 score: {}'.format(val_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(val_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(val_eval_metrics['confusion_matrix'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect test set performance, generalizability\n",
    "y_hat_test = xgb_model.predict(test_xgb)\n",
    "test_eval_metrics = eval_metrics(y_test, y_hat_test)\n",
    "\n",
    "print('-'*40)\n",
    "print('Test evaluation metrics')\n",
    "print('F1 score: {}'.format(test_eval_metrics['f1_score']))\n",
    "print('AUC for Precision-Recall Curve: {}'.format(val_eval_metrics['pr_auc']))\n",
    "print('Confusion matrix:')\n",
    "print(test_eval_metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine non-linear relationships between features and outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine feature importance for XGB model\n",
    "\n",
    "# Get the impurity-based importance values for each feature\n",
    "feature_imp = pd.DataFrame({'feature': [], 'importance': []})\n",
    "for k, v in xgb_model.get_score(importance_type='gain').items():\n",
    "    feature_imp = feature_imp.append({'feature': k, 'importance': v}, ignore_index = True)\n",
    "\n",
    "# Normalize importance values\n",
    "feature_imp['importance'] = (feature_imp['importance'] / np.sum(feature_imp['importance']))\n",
    "feature_imp = feature_imp.sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(feature_imp.iloc[0:30, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model, associated hyperparameter values, and test set eval metrics\n",
    "model = {\n",
    "    'model': xgb_model,\n",
    "    'model_hyperparameters': xgb_optimal_hp,\n",
    "    'test_set_evaluation_metrics': test_eval_metrics}\n",
    "\n",
    "# Save to pickle file\n",
    "pickle.dump(model, open(\"insurance_cross_sell_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
